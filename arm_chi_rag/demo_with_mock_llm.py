"""
æ¼”ç¤ºè„šæœ¬ï¼šå±•ç¤ºå¯ç”¨LLMåçš„RAGé—®ç­”æ•ˆæœï¼ˆä½¿ç”¨æ¨¡æ‹ŸLLMï¼‰
"""

from cursor_integration import ARMCHIRAG
from rag_system.generator import AnswerGenerator
from rag_system.rag_chain import RAGChain


class MockLLM:
    """æ¨¡æ‹ŸLLMï¼Œç”¨äºæ¼”ç¤º"""
    
    def invoke(self, prompt):
        """æ¨¡æ‹ŸLLMè°ƒç”¨"""
        class MockResponse:
            def __init__(self, content):
                self.content = content
        
        # ä»æç¤ºè¯ä¸­æå–é—®é¢˜å’Œä¸Šä¸‹æ–‡
        if "ç”¨æˆ·é—®é¢˜ï¼š" in prompt:
            query = prompt.split("ç”¨æˆ·é—®é¢˜ï¼š")[1].split("\n")[0].strip()
            context = prompt.split("æ–‡æ¡£å†…å®¹ï¼š")[1].split("ç”¨æˆ·é—®é¢˜ï¼š")[0].strip()
            
            # ç”Ÿæˆæ¨¡æ‹Ÿå›ç­”
            answer = f"""åŸºäºARM CHIæ–‡æ¡£ï¼Œå…³äº"{query}"çš„å›ç­”ï¼š

æ ¹æ®æ–‡æ¡£å†…å®¹ï¼ŒARM CHIåè®®å®šä¹‰äº†å¤šç§äº‹åŠ¡ç±»å‹ï¼ŒåŒ…æ‹¬ï¼š

1. **è¯»å–äº‹åŠ¡ï¼ˆRead Transactionsï¼‰**ï¼šç”¨äºä»å†…å­˜è¯»å–æ•°æ®
2. **å†™å…¥äº‹åŠ¡ï¼ˆWrite Transactionsï¼‰**ï¼šç”¨äºå‘å†…å­˜å†™å…¥æ•°æ®
3. **åŸå­äº‹åŠ¡ï¼ˆAtomic Transactionsï¼‰**ï¼šä¿è¯æ“ä½œçš„åŸå­æ€§
4. **ç¼“å­˜ç»´æŠ¤äº‹åŠ¡ï¼ˆCache Maintenance Transactionsï¼‰**ï¼šç”¨äºç¼“å­˜ä¸€è‡´æ€§ç»´æŠ¤

è¿™äº›äº‹åŠ¡ç±»å‹åœ¨æ–‡æ¡£ä¸­æœ‰è¯¦ç»†è¯´æ˜ï¼Œå…·ä½“å®ç°ç»†èŠ‚è¯·å‚è€ƒç›¸å…³ç« èŠ‚ã€‚

æ³¨æ„ï¼šè¿™æ˜¯æ¨¡æ‹Ÿå›ç­”ï¼Œå®é™…å›ç­”ä¼šåŸºäºçœŸå®çš„æ–‡æ¡£å†…å®¹ç”Ÿæˆã€‚"""
            
            return MockResponse(answer)
        
        return MockResponse("æ— æ³•ç†è§£é—®é¢˜")


def demo_with_mock_llm():
    """ä½¿ç”¨æ¨¡æ‹ŸLLMæ¼”ç¤ºRAGåŠŸèƒ½"""
    print("=" * 60)
    print("æ¼”ç¤ºï¼šå¯ç”¨LLMåçš„RAGé—®ç­”æ•ˆæœ")
    print("=" * 60)
    print()
    
    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_base = ARMCHIRAG()
    
    # åˆ›å»ºæ¨¡æ‹ŸLLMå’Œç”Ÿæˆå™¨
    mock_llm = MockLLM()
    generator = AnswerGenerator(llm=mock_llm)
    
    # åˆ›å»ºRAGé“¾
    rag_chain = RAGChain(
        retriever=rag_base.retriever,
        generator=generator
    )
    
    # æµ‹è¯•é—®é¢˜
    query = "ARM CHIåè®®çš„äº‹åŠ¡ç±»å‹æœ‰å“ªäº›ï¼Ÿ"
    print(f"é—®é¢˜: {query}\n")
    print("-" * 60)
    
    # æ‰§è¡ŒRAGæŸ¥è¯¢
    result = rag_chain.query(query, top_k=3, generate_answer=True)
    
    # æ˜¾ç¤ºç»“æœ
    print("ğŸ“‹ æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£:")
    for i, doc in enumerate(result['retrieved_docs'][:3], 1):
        print(f"  {i}. {doc['file_name']} ç¬¬ {doc['page']} é¡µ (ç›¸ä¼¼åº¦: {doc['score']:.4f})")
    
    print("\n" + "-" * 60)
    print("ğŸ¤– LLMç”Ÿæˆçš„å›ç­”:")
    print("-" * 60)
    if result.get('answer'):
        print(result['answer'])
    else:
        print("æœªç”Ÿæˆå›ç­”")
    
    print("\n" + "-" * 60)
    print("ğŸ“š å›ç­”æ¥æº:")
    print("-" * 60)
    for source in result.get('sources', []):
        print(f"  - {source['file_name']} ç¬¬ {source['page']} é¡µ")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ è¯´æ˜:")
    print("=" * 60)
    print("è¿™æ˜¯ä½¿ç”¨æ¨¡æ‹ŸLLMçš„æ¼”ç¤ºã€‚è¦ä½¿ç”¨çœŸå®çš„LLMï¼Œè¯·ï¼š")
    print("1. é…ç½® config.yaml ä¸­çš„ generation éƒ¨åˆ†")
    print("2. é€‰æ‹©LLMæä¾›å•†ï¼ˆOpenAIã€Ollamaç­‰ï¼‰")
    print("3. å®‰è£…ç›¸åº”çš„ä¾èµ–åŒ…")
    print("4. è®¾ç½®APIå¯†é’¥ï¼ˆå¦‚æœä½¿ç”¨äº‘æœåŠ¡ï¼‰")
    print("\nè¯¦ç»†é…ç½®è¯·æŸ¥çœ‹ RAG_VS_RETRIEVAL.md")


def show_configuration_guide():
    """æ˜¾ç¤ºé…ç½®æŒ‡å—"""
    print("\n" + "=" * 60)
    print("LLMé…ç½®æŒ‡å—")
    print("=" * 60)
    
    print("\næ–¹æ¡ˆ1ï¼šä½¿ç”¨OpenAI APIï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰")
    print("-" * 60)
    print("""
1. è·å–OpenAI APIå¯†é’¥ï¼šhttps://platform.openai.com/api-keys

2. ä¿®æ”¹ config.yamlï¼š
   generation:
     enabled: true
     provider: "openai"
     model_name: "gpt-3.5-turbo"
     api_key: "sk-your-api-key-here"
     temperature: 0.7
     max_tokens: 1000

3. å®‰è£…ä¾èµ–ï¼š
   pip install langchain-openai

4. æµ‹è¯•ï¼š
   python example_rag_usage.py
""")
    
    print("\næ–¹æ¡ˆ2ï¼šä½¿ç”¨æœ¬åœ°Ollamaï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰")
    print("-" * 60)
    print("""
1. å®‰è£…Ollamaï¼š
   # ä» https://ollama.ai ä¸‹è½½å®‰è£…
   # æˆ–ä½¿ç”¨ï¼šcurl -fsSL https://ollama.ai/install.sh | sh

2. ä¸‹è½½æ¨¡å‹ï¼š
   ollama pull llama2
   # æˆ–
   ollama pull mistral

3. ä¿®æ”¹ config.yamlï¼š
   generation:
     enabled: true
     provider: "ollama"
     model_name: "llama2"
     base_url: "http://localhost:11434"
     temperature: 0.7

4. å®‰è£…ä¾èµ–ï¼š
   pip install langchain-community

5. æµ‹è¯•ï¼š
   python example_rag_usage.py
""")
    
    print("\næ–¹æ¡ˆ3ï¼šä½¿ç”¨å…¶ä»–æœ¬åœ°æ¨¡å‹ï¼ˆOpenAIå…¼å®¹APIï¼‰")
    print("-" * 60)
    print("""
å¦‚æœæ‚¨çš„æœ¬åœ°æ¨¡å‹æä¾›OpenAIå…¼å®¹çš„APIï¼š

1. ä¿®æ”¹ config.yamlï¼š
   generation:
     enabled: true
     provider: "openai"
     model_name: "your-model-name"
     base_url: "http://localhost:8000/v1"
     api_key: "not-needed"

2. å®‰è£…ä¾èµ–ï¼š
   pip install langchain-openai
""")


if __name__ == "__main__":
    try:
        demo_with_mock_llm()
        show_configuration_guide()
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

